{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71ee78c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing files in: C:\\Users\\schildress\\OneDrive - Metropolitan Transportation Commission\\Documents\\test_outputs\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Directory containing the model output files\n",
    "data_directory = r\"C:\\Users\\schildress\\OneDrive - Metropolitan Transportation Commission\\Documents\\test_outputs\"\n",
    "\n",
    "print(f\"Analyzing files in: {data_directory}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61df0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_info(file_path: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze a single file and return its schema information\n",
    "    \"\"\"\n",
    "    file_info = {\n",
    "        'filename': file_path.name,\n",
    "        'file_size_mb': round(file_path.stat().st_size / (1024 * 1024), 2),\n",
    "        'file_extension': file_path.suffix.lower(),\n",
    "        'columns': [],\n",
    "        'data_types': {},\n",
    "        'shape': None,\n",
    "        'sample_data': None,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Determine file type and read accordingly\n",
    "        if file_path.suffix.lower() == '.csv':\n",
    "            # Read just the first few rows to get schema\n",
    "            df = pd.read_csv(file_path, nrows=1000)\n",
    "        elif file_path.suffix.lower() in ['.xlsx', '.xls']:\n",
    "            df = pd.read_excel(file_path, nrows=1000)\n",
    "        elif file_path.suffix.lower() == '.parquet':\n",
    "            df = pd.read_parquet(file_path)\n",
    "            # For parquet, just take first 1000 rows for sample\n",
    "            df = df.head(1000)\n",
    "        elif file_path.suffix.lower() in ['.txt', '.tsv']:\n",
    "            # Try tab-separated first, then comma\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep='\\t', nrows=1000)\n",
    "            except:\n",
    "                df = pd.read_csv(file_path, nrows=1000)\n",
    "        else:\n",
    "            # Try to read as CSV by default\n",
    "            df = pd.read_csv(file_path, nrows=1000)\n",
    "        \n",
    "        # Extract schema information\n",
    "        file_info['columns'] = list(df.columns)\n",
    "        file_info['data_types'] = {col: str(dtype) for col, dtype in df.dtypes.items()}\n",
    "        file_info['shape'] = df.shape\n",
    "        file_info['sample_data'] = df.head(3).to_dict('records')\n",
    "        \n",
    "    except Exception as e:\n",
    "        file_info['error'] = str(e)\n",
    "    \n",
    "    return file_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b64c5159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_directory_schema(directory_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze all files in the directory and return comprehensive schema information\n",
    "    \"\"\"\n",
    "    data_dir = Path(directory_path)\n",
    "    \n",
    "    if not data_dir.exists():\n",
    "        return {'error': f\"Directory {directory_path} does not exist\"}\n",
    "    \n",
    "    # Find all data files (common formats)\n",
    "    data_extensions = {'.csv', '.xlsx', '.xls', '.parquet', '.txt', '.tsv', '.json'}\n",
    "    data_files = []\n",
    "    \n",
    "    for ext in data_extensions:\n",
    "        data_files.extend(list(data_dir.glob(f\"*{ext}\")))\n",
    "        data_files.extend(list(data_dir.glob(f\"**/*{ext}\")))  # Include subdirectories\n",
    "    \n",
    "    print(f\"Found {len(data_files)} data files\")\n",
    "    \n",
    "    # Analyze each file\n",
    "    file_schemas = []\n",
    "    for file_path in data_files:\n",
    "        print(f\"Analyzing: {file_path.name}\")\n",
    "        schema_info = get_file_info(file_path)\n",
    "        file_schemas.append(schema_info)\n",
    "    \n",
    "    return {\n",
    "        'directory': str(data_dir),\n",
    "        'total_files': len(data_files),\n",
    "        'file_schemas': file_schemas\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63287664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 data files\n",
      "Analyzing: householdData_1.csv\n",
      "Analyzing: indivTourData_1.csv\n",
      "Analyzing: indivTripData_1.csv\n",
      "Analyzing: jointTourData_1.csv\n",
      "Analyzing: jointTripData_1.csv\n",
      "Analyzing: personData_1.csv\n",
      "Analyzing: householdData_1.csv\n",
      "Analyzing: indivTourData_1.csv\n",
      "Analyzing: indivTripData_1.csv\n",
      "Analyzing: jointTourData_1.csv\n",
      "Analyzing: jointTripData_1.csv\n",
      "Analyzing: personData_1.csv\n"
     ]
    }
   ],
   "source": [
    "# Run the schema analysis\n",
    "schema_results = analyze_directory_schema(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f88bb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ DIRECTORY ANALYSIS SUMMARY\n",
      "Directory: C:\\Users\\schildress\\OneDrive - Metropolitan Transportation Commission\\Documents\\test_outputs\n",
      "Total Files Found: 10\n",
      "================================================================================\n",
      "\n",
      "üìÑ FILE 1: householdData_1.csv\n",
      "   Size: 7.73 MB\n",
      "   Extension: .csv\n",
      "   Shape: (1000, 12) (rows, columns)\n",
      "   Columns (12):\n",
      "      ‚Ä¢ hh_id                          (int64)\n",
      "      ‚Ä¢ home_mgra                      (int64)\n",
      "      ‚Ä¢ income                         (int64)\n",
      "      ‚Ä¢ autos                          (int64)\n",
      "      ‚Ä¢ automated_vehicles             (int64)\n",
      "      ‚Ä¢ transponder                    (int64)\n",
      "      ‚Ä¢ pre_et_cdap_pattern            (object)\n",
      "      ‚Ä¢ cdap_pattern                   (object)\n",
      "      ‚Ä¢ jtf_choice                     (int64)\n",
      "      ‚Ä¢ sampleRate                     (float64)\n",
      "      ‚Ä¢ size                           (int64)\n",
      "      ‚Ä¢ workers                        (int64)\n",
      "   Sample Data (first row):\n",
      "      hh_id: 1581987\n",
      "      home_mgra: 1\n",
      "      income: 44257\n",
      "      autos: 2\n",
      "      automated_vehicles: 0\n",
      "      ... and 7 more columns\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ FILE 2: indivTourData_1.csv\n",
      "   Size: 172.95 MB\n",
      "   Extension: .csv\n",
      "   Shape: (1000, 54) (rows, columns)\n",
      "   Columns (54):\n",
      "      ‚Ä¢ hh_id                          (int64)\n",
      "      ‚Ä¢ person_id                      (int64)\n",
      "      ‚Ä¢ person_num                     (int64)\n",
      "      ‚Ä¢ person_type                    (int64)\n",
      "      ‚Ä¢ tour_id                        (int64)\n",
      "      ‚Ä¢ tour_category                  (object)\n",
      "      ‚Ä¢ tour_purpose                   (object)\n",
      "      ‚Ä¢ orig_mgra                      (int64)\n",
      "      ‚Ä¢ dest_mgra                      (int64)\n",
      "      ‚Ä¢ start_period                   (int64)\n",
      "      ‚Ä¢ end_period                     (int64)\n",
      "      ‚Ä¢ tour_mode                      (int64)\n",
      "      ‚Ä¢ tour_distance                  (float64)\n",
      "      ‚Ä¢ tour_time                      (float64)\n",
      "      ‚Ä¢ atWork_freq                    (int64)\n",
      "      ‚Ä¢ num_ob_stops                   (int64)\n",
      "      ‚Ä¢ num_ib_stops                   (int64)\n",
      "      ‚Ä¢ sampleRate                     (float64)\n",
      "      ‚Ä¢ avAvailable                    (int64)\n",
      "      ‚Ä¢ dcLogsum                       (float64)\n",
      "      ‚Ä¢ util_1                         (float64)\n",
      "      ‚Ä¢ util_2                         (float64)\n",
      "      ‚Ä¢ util_3                         (float64)\n",
      "      ‚Ä¢ util_4                         (float64)\n",
      "      ‚Ä¢ util_5                         (float64)\n",
      "      ‚Ä¢ util_6                         (float64)\n",
      "      ‚Ä¢ util_7                         (float64)\n",
      "      ‚Ä¢ util_8                         (float64)\n",
      "      ‚Ä¢ util_9                         (float64)\n",
      "      ‚Ä¢ util_10                        (float64)\n",
      "      ‚Ä¢ util_11                        (float64)\n",
      "      ‚Ä¢ util_12                        (float64)\n",
      "      ‚Ä¢ util_13                        (float64)\n",
      "      ‚Ä¢ util_14                        (float64)\n",
      "      ‚Ä¢ util_15                        (float64)\n",
      "      ‚Ä¢ util_16                        (float64)\n",
      "      ‚Ä¢ util_17                        (float64)\n",
      "      ‚Ä¢ prob_1                         (float64)\n",
      "      ‚Ä¢ prob_2                         (float64)\n",
      "      ‚Ä¢ prob_3                         (float64)\n",
      "      ‚Ä¢ prob_4                         (float64)\n",
      "      ‚Ä¢ prob_5                         (float64)\n",
      "      ‚Ä¢ prob_6                         (float64)\n",
      "      ‚Ä¢ prob_7                         (float64)\n",
      "      ‚Ä¢ prob_8                         (float64)\n",
      "      ‚Ä¢ prob_9                         (float64)\n",
      "      ‚Ä¢ prob_10                        (float64)\n",
      "      ‚Ä¢ prob_11                        (float64)\n",
      "      ‚Ä¢ prob_12                        (float64)\n",
      "      ‚Ä¢ prob_13                        (float64)\n",
      "      ‚Ä¢ prob_14                        (float64)\n",
      "      ‚Ä¢ prob_15                        (float64)\n",
      "      ‚Ä¢ prob_16                        (float64)\n",
      "      ‚Ä¢ prob_17                        (float64)\n",
      "   Sample Data (first row):\n",
      "      hh_id: 1581987\n",
      "      person_id: 3778450\n",
      "      person_num: 1\n",
      "      person_type: 4\n",
      "      tour_id: 0\n",
      "      ... and 49 more columns\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ FILE 3: indivTripData_1.csv\n",
      "   Size: 110.95 MB\n",
      "   Extension: .csv\n",
      "   Shape: (1000, 19) (rows, columns)\n",
      "   Columns (19):\n",
      "      ‚Ä¢ hh_id                          (int64)\n",
      "      ‚Ä¢ person_id                      (int64)\n",
      "      ‚Ä¢ person_num                     (int64)\n",
      "      ‚Ä¢ tour_id                        (int64)\n",
      "      ‚Ä¢ stop_id                        (int64)\n",
      "      ‚Ä¢ inbound                        (int64)\n",
      "      ‚Ä¢ tour_purpose                   (object)\n",
      "      ‚Ä¢ orig_purpose                   (object)\n",
      "      ‚Ä¢ dest_purpose                   (object)\n",
      "      ‚Ä¢ orig_mgra                      (int64)\n",
      "      ‚Ä¢ dest_mgra                      (int64)\n",
      "      ‚Ä¢ trip_dist                      (float64)\n",
      "      ‚Ä¢ parking_mgra                   (int64)\n",
      "      ‚Ä¢ stop_period                    (int64)\n",
      "      ‚Ä¢ trip_mode                      (int64)\n",
      "      ‚Ä¢ tour_mode                      (int64)\n",
      "      ‚Ä¢ tranpath_rnum                  (float64)\n",
      "      ‚Ä¢ sampleRate                     (float64)\n",
      "      ‚Ä¢ avAvailable                    (int64)\n",
      "   Sample Data (first row):\n",
      "      hh_id: 1581987\n",
      "      person_id: 3778450\n",
      "      person_num: 1\n",
      "      tour_id: 0\n",
      "      stop_id: -1\n",
      "      ... and 14 more columns\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ FILE 4: jointTourData_1.csv\n",
      "   Size: 16.54 MB\n",
      "   Extension: .csv\n",
      "   Shape: (1000, 52) (rows, columns)\n",
      "   Columns (52):\n",
      "      ‚Ä¢ hh_id                          (int64)\n",
      "      ‚Ä¢ tour_id                        (int64)\n",
      "      ‚Ä¢ tour_category                  (object)\n",
      "      ‚Ä¢ tour_purpose                   (object)\n",
      "      ‚Ä¢ tour_composition               (int64)\n",
      "      ‚Ä¢ tour_participants              (object)\n",
      "      ‚Ä¢ orig_mgra                      (int64)\n",
      "      ‚Ä¢ dest_mgra                      (int64)\n",
      "      ‚Ä¢ start_period                   (int64)\n",
      "      ‚Ä¢ end_period                     (int64)\n",
      "      ‚Ä¢ tour_mode                      (int64)\n",
      "      ‚Ä¢ tour_distance                  (float64)\n",
      "      ‚Ä¢ tour_time                      (float64)\n",
      "      ‚Ä¢ num_ob_stops                   (int64)\n",
      "      ‚Ä¢ num_ib_stops                   (int64)\n",
      "      ‚Ä¢ sampleRate                     (float64)\n",
      "      ‚Ä¢ avAvailable                    (int64)\n",
      "      ‚Ä¢ dcLogsum                       (float64)\n",
      "      ‚Ä¢ util_1                         (float64)\n",
      "      ‚Ä¢ util_2                         (float64)\n",
      "      ‚Ä¢ util_3                         (float64)\n",
      "      ‚Ä¢ util_4                         (float64)\n",
      "      ‚Ä¢ util_5                         (float64)\n",
      "      ‚Ä¢ util_6                         (float64)\n",
      "      ‚Ä¢ util_7                         (float64)\n",
      "      ‚Ä¢ util_8                         (float64)\n",
      "      ‚Ä¢ util_9                         (float64)\n",
      "      ‚Ä¢ util_10                        (float64)\n",
      "      ‚Ä¢ util_11                        (float64)\n",
      "      ‚Ä¢ util_12                        (float64)\n",
      "      ‚Ä¢ util_13                        (float64)\n",
      "      ‚Ä¢ util_14                        (float64)\n",
      "      ‚Ä¢ util_15                        (float64)\n",
      "      ‚Ä¢ util_16                        (int64)\n",
      "      ‚Ä¢ util_17                        (float64)\n",
      "      ‚Ä¢ prob_1                         (float64)\n",
      "      ‚Ä¢ prob_2                         (float64)\n",
      "      ‚Ä¢ prob_3                         (float64)\n",
      "      ‚Ä¢ prob_4                         (float64)\n",
      "      ‚Ä¢ prob_5                         (float64)\n",
      "      ‚Ä¢ prob_6                         (float64)\n",
      "      ‚Ä¢ prob_7                         (float64)\n",
      "      ‚Ä¢ prob_8                         (float64)\n",
      "      ‚Ä¢ prob_9                         (float64)\n",
      "      ‚Ä¢ prob_10                        (float64)\n",
      "      ‚Ä¢ prob_11                        (float64)\n",
      "      ‚Ä¢ prob_12                        (float64)\n",
      "      ‚Ä¢ prob_13                        (float64)\n",
      "      ‚Ä¢ prob_14                        (float64)\n",
      "      ‚Ä¢ prob_15                        (float64)\n",
      "      ‚Ä¢ prob_16                        (float64)\n",
      "      ‚Ä¢ prob_17                        (float64)\n",
      "   Sample Data (first row):\n",
      "      hh_id: 1581987\n",
      "      tour_id: 0\n",
      "      tour_category: JOINT_NON_MANDATORY\n",
      "      tour_purpose: Visiting\n",
      "      tour_composition: 3\n",
      "      ... and 47 more columns\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ FILE 5: jointTripData_1.csv\n",
      "   Size: 9.81 MB\n",
      "   Extension: .csv\n",
      "   Shape: (1000, 18) (rows, columns)\n",
      "   Columns (18):\n",
      "      ‚Ä¢ hh_id                          (int64)\n",
      "      ‚Ä¢ tour_id                        (int64)\n",
      "      ‚Ä¢ stop_id                        (int64)\n",
      "      ‚Ä¢ inbound                        (int64)\n",
      "      ‚Ä¢ tour_purpose                   (object)\n",
      "      ‚Ä¢ orig_purpose                   (object)\n",
      "      ‚Ä¢ dest_purpose                   (object)\n",
      "      ‚Ä¢ orig_mgra                      (int64)\n",
      "      ‚Ä¢ dest_mgra                      (int64)\n",
      "      ‚Ä¢ trip_dist                      (float64)\n",
      "      ‚Ä¢ parking_mgra                   (int64)\n",
      "      ‚Ä¢ stop_period                    (int64)\n",
      "      ‚Ä¢ trip_mode                      (int64)\n",
      "      ‚Ä¢ num_participants               (int64)\n",
      "      ‚Ä¢ tour_mode                      (int64)\n",
      "      ‚Ä¢ tranpath_rnum                  (float64)\n",
      "      ‚Ä¢ sampleRate                     (float64)\n",
      "      ‚Ä¢ avAvailable                    (int64)\n",
      "   Sample Data (first row):\n",
      "      hh_id: 1581987\n",
      "      tour_id: 0\n",
      "      stop_id: -1\n",
      "      inbound: 0\n",
      "      tour_purpose: Visiting\n",
      "      ... and 13 more columns\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ FILE 6: householdData_1.csv\n",
      "   Size: 7.73 MB\n",
      "   Extension: .csv\n",
      "   Shape: (1000, 12) (rows, columns)\n",
      "   Columns (12):\n",
      "      ‚Ä¢ hh_id                          (int64)\n",
      "      ‚Ä¢ home_mgra                      (int64)\n",
      "      ‚Ä¢ income                         (int64)\n",
      "      ‚Ä¢ autos                          (int64)\n",
      "      ‚Ä¢ automated_vehicles             (int64)\n",
      "      ‚Ä¢ transponder                    (int64)\n",
      "      ‚Ä¢ pre_et_cdap_pattern            (object)\n",
      "      ‚Ä¢ cdap_pattern                   (object)\n",
      "      ‚Ä¢ jtf_choice                     (int64)\n",
      "      ‚Ä¢ sampleRate                     (float64)\n",
      "      ‚Ä¢ size                           (int64)\n",
      "      ‚Ä¢ workers                        (int64)\n",
      "   Sample Data (first row):\n",
      "      hh_id: 1581987\n",
      "      home_mgra: 1\n",
      "      income: 44257\n",
      "      autos: 2\n",
      "      automated_vehicles: 0\n",
      "      ... and 7 more columns\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ FILE 7: indivTourData_1.csv\n",
      "   Size: 172.95 MB\n",
      "   Extension: .csv\n",
      "   Shape: (1000, 54) (rows, columns)\n",
      "   Columns (54):\n",
      "      ‚Ä¢ hh_id                          (int64)\n",
      "      ‚Ä¢ person_id                      (int64)\n",
      "      ‚Ä¢ person_num                     (int64)\n",
      "      ‚Ä¢ person_type                    (int64)\n",
      "      ‚Ä¢ tour_id                        (int64)\n",
      "      ‚Ä¢ tour_category                  (object)\n",
      "      ‚Ä¢ tour_purpose                   (object)\n",
      "      ‚Ä¢ orig_mgra                      (int64)\n",
      "      ‚Ä¢ dest_mgra                      (int64)\n",
      "      ‚Ä¢ start_period                   (int64)\n",
      "      ‚Ä¢ end_period                     (int64)\n",
      "      ‚Ä¢ tour_mode                      (int64)\n",
      "      ‚Ä¢ tour_distance                  (float64)\n",
      "      ‚Ä¢ tour_time                      (float64)\n",
      "      ‚Ä¢ atWork_freq                    (int64)\n",
      "      ‚Ä¢ num_ob_stops                   (int64)\n",
      "      ‚Ä¢ num_ib_stops                   (int64)\n",
      "      ‚Ä¢ sampleRate                     (float64)\n",
      "      ‚Ä¢ avAvailable                    (int64)\n",
      "      ‚Ä¢ dcLogsum                       (float64)\n",
      "      ‚Ä¢ util_1                         (float64)\n",
      "      ‚Ä¢ util_2                         (float64)\n",
      "      ‚Ä¢ util_3                         (float64)\n",
      "      ‚Ä¢ util_4                         (float64)\n",
      "      ‚Ä¢ util_5                         (float64)\n",
      "      ‚Ä¢ util_6                         (float64)\n",
      "      ‚Ä¢ util_7                         (float64)\n",
      "      ‚Ä¢ util_8                         (float64)\n",
      "      ‚Ä¢ util_9                         (float64)\n",
      "      ‚Ä¢ util_10                        (float64)\n",
      "      ‚Ä¢ util_11                        (float64)\n",
      "      ‚Ä¢ util_12                        (float64)\n",
      "      ‚Ä¢ util_13                        (float64)\n",
      "      ‚Ä¢ util_14                        (float64)\n",
      "      ‚Ä¢ util_15                        (float64)\n",
      "      ‚Ä¢ util_16                        (float64)\n",
      "      ‚Ä¢ util_17                        (float64)\n",
      "      ‚Ä¢ prob_1                         (float64)\n",
      "      ‚Ä¢ prob_2                         (float64)\n",
      "      ‚Ä¢ prob_3                         (float64)\n",
      "      ‚Ä¢ prob_4                         (float64)\n",
      "      ‚Ä¢ prob_5                         (float64)\n",
      "      ‚Ä¢ prob_6                         (float64)\n",
      "      ‚Ä¢ prob_7                         (float64)\n",
      "      ‚Ä¢ prob_8                         (float64)\n",
      "      ‚Ä¢ prob_9                         (float64)\n",
      "      ‚Ä¢ prob_10                        (float64)\n",
      "      ‚Ä¢ prob_11                        (float64)\n",
      "      ‚Ä¢ prob_12                        (float64)\n",
      "      ‚Ä¢ prob_13                        (float64)\n",
      "      ‚Ä¢ prob_14                        (float64)\n",
      "      ‚Ä¢ prob_15                        (float64)\n",
      "      ‚Ä¢ prob_16                        (float64)\n",
      "      ‚Ä¢ prob_17                        (float64)\n",
      "   Sample Data (first row):\n",
      "      hh_id: 1581987\n",
      "      person_id: 3778450\n",
      "      person_num: 1\n",
      "      person_type: 4\n",
      "      tour_id: 0\n",
      "      ... and 49 more columns\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ FILE 8: indivTripData_1.csv\n",
      "   Size: 110.95 MB\n",
      "   Extension: .csv\n",
      "   Shape: (1000, 19) (rows, columns)\n",
      "   Columns (19):\n",
      "      ‚Ä¢ hh_id                          (int64)\n",
      "      ‚Ä¢ person_id                      (int64)\n",
      "      ‚Ä¢ person_num                     (int64)\n",
      "      ‚Ä¢ tour_id                        (int64)\n",
      "      ‚Ä¢ stop_id                        (int64)\n",
      "      ‚Ä¢ inbound                        (int64)\n",
      "      ‚Ä¢ tour_purpose                   (object)\n",
      "      ‚Ä¢ orig_purpose                   (object)\n",
      "      ‚Ä¢ dest_purpose                   (object)\n",
      "      ‚Ä¢ orig_mgra                      (int64)\n",
      "      ‚Ä¢ dest_mgra                      (int64)\n",
      "      ‚Ä¢ trip_dist                      (float64)\n",
      "      ‚Ä¢ parking_mgra                   (int64)\n",
      "      ‚Ä¢ stop_period                    (int64)\n",
      "      ‚Ä¢ trip_mode                      (int64)\n",
      "      ‚Ä¢ tour_mode                      (int64)\n",
      "      ‚Ä¢ tranpath_rnum                  (float64)\n",
      "      ‚Ä¢ sampleRate                     (float64)\n",
      "      ‚Ä¢ avAvailable                    (int64)\n",
      "   Sample Data (first row):\n",
      "      hh_id: 1581987\n",
      "      person_id: 3778450\n",
      "      person_num: 1\n",
      "      tour_id: 0\n",
      "      stop_id: -1\n",
      "      ... and 14 more columns\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ FILE 9: jointTourData_1.csv\n",
      "   Size: 16.54 MB\n",
      "   Extension: .csv\n",
      "   Shape: (1000, 52) (rows, columns)\n",
      "   Columns (52):\n",
      "      ‚Ä¢ hh_id                          (int64)\n",
      "      ‚Ä¢ tour_id                        (int64)\n",
      "      ‚Ä¢ tour_category                  (object)\n",
      "      ‚Ä¢ tour_purpose                   (object)\n",
      "      ‚Ä¢ tour_composition               (int64)\n",
      "      ‚Ä¢ tour_participants              (object)\n",
      "      ‚Ä¢ orig_mgra                      (int64)\n",
      "      ‚Ä¢ dest_mgra                      (int64)\n",
      "      ‚Ä¢ start_period                   (int64)\n",
      "      ‚Ä¢ end_period                     (int64)\n",
      "      ‚Ä¢ tour_mode                      (int64)\n",
      "      ‚Ä¢ tour_distance                  (float64)\n",
      "      ‚Ä¢ tour_time                      (float64)\n",
      "      ‚Ä¢ num_ob_stops                   (int64)\n",
      "      ‚Ä¢ num_ib_stops                   (int64)\n",
      "      ‚Ä¢ sampleRate                     (float64)\n",
      "      ‚Ä¢ avAvailable                    (int64)\n",
      "      ‚Ä¢ dcLogsum                       (float64)\n",
      "      ‚Ä¢ util_1                         (float64)\n",
      "      ‚Ä¢ util_2                         (float64)\n",
      "      ‚Ä¢ util_3                         (float64)\n",
      "      ‚Ä¢ util_4                         (float64)\n",
      "      ‚Ä¢ util_5                         (float64)\n",
      "      ‚Ä¢ util_6                         (float64)\n",
      "      ‚Ä¢ util_7                         (float64)\n",
      "      ‚Ä¢ util_8                         (float64)\n",
      "      ‚Ä¢ util_9                         (float64)\n",
      "      ‚Ä¢ util_10                        (float64)\n",
      "      ‚Ä¢ util_11                        (float64)\n",
      "      ‚Ä¢ util_12                        (float64)\n",
      "      ‚Ä¢ util_13                        (float64)\n",
      "      ‚Ä¢ util_14                        (float64)\n",
      "      ‚Ä¢ util_15                        (float64)\n",
      "      ‚Ä¢ util_16                        (int64)\n",
      "      ‚Ä¢ util_17                        (float64)\n",
      "      ‚Ä¢ prob_1                         (float64)\n",
      "      ‚Ä¢ prob_2                         (float64)\n",
      "      ‚Ä¢ prob_3                         (float64)\n",
      "      ‚Ä¢ prob_4                         (float64)\n",
      "      ‚Ä¢ prob_5                         (float64)\n",
      "      ‚Ä¢ prob_6                         (float64)\n",
      "      ‚Ä¢ prob_7                         (float64)\n",
      "      ‚Ä¢ prob_8                         (float64)\n",
      "      ‚Ä¢ prob_9                         (float64)\n",
      "      ‚Ä¢ prob_10                        (float64)\n",
      "      ‚Ä¢ prob_11                        (float64)\n",
      "      ‚Ä¢ prob_12                        (float64)\n",
      "      ‚Ä¢ prob_13                        (float64)\n",
      "      ‚Ä¢ prob_14                        (float64)\n",
      "      ‚Ä¢ prob_15                        (float64)\n",
      "      ‚Ä¢ prob_16                        (float64)\n",
      "      ‚Ä¢ prob_17                        (float64)\n",
      "   Sample Data (first row):\n",
      "      hh_id: 1581987\n",
      "      tour_id: 0\n",
      "      tour_category: JOINT_NON_MANDATORY\n",
      "      tour_purpose: Visiting\n",
      "      tour_composition: 3\n",
      "      ... and 47 more columns\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÑ FILE 10: jointTripData_1.csv\n",
      "   Size: 9.81 MB\n",
      "   Extension: .csv\n",
      "   Shape: (1000, 18) (rows, columns)\n",
      "   Columns (18):\n",
      "      ‚Ä¢ hh_id                          (int64)\n",
      "      ‚Ä¢ tour_id                        (int64)\n",
      "      ‚Ä¢ stop_id                        (int64)\n",
      "      ‚Ä¢ inbound                        (int64)\n",
      "      ‚Ä¢ tour_purpose                   (object)\n",
      "      ‚Ä¢ orig_purpose                   (object)\n",
      "      ‚Ä¢ dest_purpose                   (object)\n",
      "      ‚Ä¢ orig_mgra                      (int64)\n",
      "      ‚Ä¢ dest_mgra                      (int64)\n",
      "      ‚Ä¢ trip_dist                      (float64)\n",
      "      ‚Ä¢ parking_mgra                   (int64)\n",
      "      ‚Ä¢ stop_period                    (int64)\n",
      "      ‚Ä¢ trip_mode                      (int64)\n",
      "      ‚Ä¢ num_participants               (int64)\n",
      "      ‚Ä¢ tour_mode                      (int64)\n",
      "      ‚Ä¢ tranpath_rnum                  (float64)\n",
      "      ‚Ä¢ sampleRate                     (float64)\n",
      "      ‚Ä¢ avAvailable                    (int64)\n",
      "   Sample Data (first row):\n",
      "      hh_id: 1581987\n",
      "      tour_id: 0\n",
      "      stop_id: -1\n",
      "      inbound: 0\n",
      "      tour_purpose: Visiting\n",
      "      ... and 13 more columns\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_schema_summary(results: Dict):\n",
    "    \"\"\"\n",
    "    Print a formatted summary of the schema analysis results\n",
    "    \"\"\"\n",
    "    if 'error' in results:\n",
    "        print(f\"Error: {results['error']}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìÅ DIRECTORY ANALYSIS SUMMARY\")\n",
    "    print(f\"Directory: {results['directory']}\")\n",
    "    print(f\"Total Files Found: {results['total_files']}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, file_info in enumerate(results['file_schemas'], 1):\n",
    "        print(f\"\\nüìÑ FILE {i}: {file_info['filename']}\")\n",
    "        print(f\"   Size: {file_info['file_size_mb']} MB\")\n",
    "        print(f\"   Extension: {file_info['file_extension']}\")\n",
    "        \n",
    "        if file_info['error']:\n",
    "            print(f\"   ‚ùå Error: {file_info['error']}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"   Shape: {file_info['shape']} (rows, columns)\")\n",
    "        print(f\"   Columns ({len(file_info['columns'])}):\")\n",
    "        \n",
    "        # Print columns and data types in a nice format\n",
    "        for col in file_info['columns']:\n",
    "            dtype = file_info['data_types'].get(col, 'unknown')\n",
    "            print(f\"      ‚Ä¢ {col:<30} ({dtype})\")\n",
    "        \n",
    "        # Show sample data if available\n",
    "        if file_info['sample_data'] and len(file_info['sample_data']) > 0:\n",
    "            print(f\"   Sample Data (first row):\")\n",
    "            first_row = file_info['sample_data'][0]\n",
    "            for key, value in list(first_row.items())[:5]:  # Show first 5 columns\n",
    "                print(f\"      {key}: {value}\")\n",
    "            if len(first_row) > 5:\n",
    "                print(f\"      ... and {len(first_row)-5} more columns\")\n",
    "        \n",
    "        print(\"-\"*60)\n",
    "\n",
    "# Print the results\n",
    "print_schema_summary(schema_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dbee108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä CONSOLIDATED SCHEMA DATA\n",
      "Total schema entries: 310\n",
      "\n",
      "First 10 entries:\n",
      "              filename  file_size_mb file_extension          column_name  \\\n",
      "0  householdData_1.csv          7.73           .csv                hh_id   \n",
      "1  householdData_1.csv          7.73           .csv            home_mgra   \n",
      "2  householdData_1.csv          7.73           .csv               income   \n",
      "3  householdData_1.csv          7.73           .csv                autos   \n",
      "4  householdData_1.csv          7.73           .csv   automated_vehicles   \n",
      "5  householdData_1.csv          7.73           .csv          transponder   \n",
      "6  householdData_1.csv          7.73           .csv  pre_et_cdap_pattern   \n",
      "7  householdData_1.csv          7.73           .csv         cdap_pattern   \n",
      "8  householdData_1.csv          7.73           .csv           jtf_choice   \n",
      "9  householdData_1.csv          7.73           .csv           sampleRate   \n",
      "\n",
      "  data_type error  total_columns  total_rows  \n",
      "0     int64  None             12        1000  \n",
      "1     int64  None             12        1000  \n",
      "2     int64  None             12        1000  \n",
      "3     int64  None             12        1000  \n",
      "4     int64  None             12        1000  \n",
      "5     int64  None             12        1000  \n",
      "6    object  None             12        1000  \n",
      "7    object  None             12        1000  \n",
      "8     int64  None             12        1000  \n",
      "9   float64  None             12        1000  \n"
     ]
    }
   ],
   "source": [
    "# Create a consolidated schema DataFrame for easy export/analysis\n",
    "def create_schema_dataframe(results: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a consolidated DataFrame with all schema information\n",
    "    \"\"\"\n",
    "    schema_rows = []\n",
    "    \n",
    "    for file_info in results['file_schemas']:\n",
    "        if file_info['error']:\n",
    "            schema_rows.append({\n",
    "                'filename': file_info['filename'],\n",
    "                'file_size_mb': file_info['file_size_mb'],\n",
    "                'file_extension': file_info['file_extension'],\n",
    "                'column_name': None,\n",
    "                'data_type': None,\n",
    "                'error': file_info['error'],\n",
    "                'total_columns': None,\n",
    "                'total_rows': None\n",
    "            })\n",
    "        else:\n",
    "            for col in file_info['columns']:\n",
    "                schema_rows.append({\n",
    "                    'filename': file_info['filename'],\n",
    "                    'file_size_mb': file_info['file_size_mb'],\n",
    "                    'file_extension': file_info['file_extension'],\n",
    "                    'column_name': col,\n",
    "                    'data_type': file_info['data_types'].get(col, 'unknown'),\n",
    "                    'error': None,\n",
    "                    'total_columns': len(file_info['columns']),\n",
    "                    'total_rows': file_info['shape'][0] if file_info['shape'] else None\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(schema_rows)\n",
    "\n",
    "# Create the consolidated schema DataFrame\n",
    "schema_df = create_schema_dataframe(schema_results)\n",
    "print(f\"\\nüìä CONSOLIDATED SCHEMA DATA\")\n",
    "print(f\"Total schema entries: {len(schema_df)}\")\n",
    "print(\"\\nFirst 10 entries:\")\n",
    "print(schema_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e66a6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Schema information exported to: ctramp_2023_data_schema.csv\n",
      "\n",
      "üìà SUMMARY STATISTICS\n",
      "Unique files analyzed: 5\n",
      "Total data types found: 3\n",
      "File types: {'.csv': 310}\n",
      "\n",
      "üè∑Ô∏è MOST COMMON COLUMN NAMES:\n",
      "   hh_id: appears in 10 files\n",
      "   sampleRate: appears in 10 files\n",
      "   dest_mgra: appears in 8 files\n",
      "   orig_mgra: appears in 8 files\n",
      "   tour_purpose: appears in 8 files\n",
      "   tour_id: appears in 8 files\n",
      "   avAvailable: appears in 8 files\n",
      "   tour_mode: appears in 8 files\n",
      "   prob_3: appears in 4 files\n",
      "   prob_2: appears in 4 files\n",
      "\n",
      "üìä DATA TYPE DISTRIBUTION:\n",
      "   float64: 164 columns\n",
      "   int64: 120 columns\n",
      "   object: 26 columns\n"
     ]
    }
   ],
   "source": [
    "# Export schema information to CSV for future reference\n",
    "output_file = \"ctramp_2023_data_schema.csv\"\n",
    "schema_df.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Schema information exported to: {output_file}\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(f\"\\nüìà SUMMARY STATISTICS\")\n",
    "print(f\"Unique files analyzed: {schema_df['filename'].nunique()}\")\n",
    "print(f\"Total data types found: {schema_df['data_type'].nunique()}\")\n",
    "print(f\"File types: {schema_df['file_extension'].value_counts().to_dict()}\")\n",
    "\n",
    "# Show most common column names (could indicate standard fields)\n",
    "print(f\"\\nüè∑Ô∏è MOST COMMON COLUMN NAMES:\")\n",
    "common_columns = schema_df['column_name'].value_counts().head(10)\n",
    "for col_name, count in common_columns.items():\n",
    "    if pd.notna(col_name):\n",
    "        print(f\"   {col_name}: appears in {count} files\")\n",
    "\n",
    "# Show data type distribution\n",
    "print(f\"\\nüìä DATA TYPE DISTRIBUTION:\")\n",
    "dtype_counts = schema_df['data_type'].value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    if pd.notna(dtype):\n",
    "        print(f\"   {dtype}: {count} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d060e2b2",
   "metadata": {},
   "source": [
    "# CTRAMP 2023 Model Output Schema Analysis\n",
    "\n",
    "This notebook analyzes the data schema of Bay Area residential travel demand model output files from 2023. The script:\n",
    "\n",
    "1. **Scans the specified directory** for common data file formats (CSV, Excel, Parquet, etc.)\n",
    "2. **Extracts schema information** including:\n",
    "   - Column names and data types\n",
    "   - File sizes and shapes\n",
    "   - Sample data from each file\n",
    "3. **Creates a consolidated summary** of all schemas\n",
    "4. **Exports results** to CSV for further analysis\n",
    "\n",
    "## Key Features:\n",
    "- Handles multiple file formats automatically\n",
    "- Provides detailed error reporting for problematic files\n",
    "- Creates summary statistics about common columns and data types\n",
    "- Exports all findings to a structured CSV file\n",
    "\n",
    "## Usage:\n",
    "Simply update the `data_directory` variable in the first cell to point to your model output folder and run all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2c2a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CTRAMP documentation schemas loaded!\n",
      "üìã Defined schemas for 13 file types\n",
      "üöó Mode codes: 17 modes defined\n",
      "‚è∞ Time periods: 40 periods defined\n"
     ]
    }
   ],
   "source": [
    "# CTRAMP Documentation Integration\n",
    "# Define expected schemas based on official CTRAMP documentation\n",
    "\n",
    "CTRAMP_FILE_SCHEMAS = {\n",
    "    'accessibilities.csv': {\n",
    "        'description': 'Various accessibility measures by microzone',\n",
    "        'expected_fields': ['mgra'],  # Basic field, actual schema varies\n",
    "        'file_type': 'accessibility'\n",
    "    },\n",
    "    \n",
    "    'aoResults_pre.csv': {\n",
    "        'description': 'Household auto ownership before work and school location choice',\n",
    "        'expected_fields': ['hh_id'],\n",
    "        'file_type': 'auto_ownership'\n",
    "    },\n",
    "    \n",
    "    'aoResults.csv': {\n",
    "        'description': 'Household auto ownership after work and school location choice',\n",
    "        'expected_fields': ['hh_id'],\n",
    "        'file_type': 'auto_ownership'\n",
    "    },\n",
    "    \n",
    "    'householdData': {\n",
    "        'description': 'Household level model results',\n",
    "        'expected_fields': ['hh_id', 'transponder', 'cdap_pattern', 'jtf_choice'],\n",
    "        'file_type': 'household'\n",
    "    },\n",
    "    \n",
    "    'personData': {\n",
    "        'description': 'Person level model results',\n",
    "        'expected_fields': ['hh_id', 'person_id', 'person_num', 'value_of_time', \n",
    "                          'transitSubsidy_choice', 'transitSubsidy_percent', 'transitPass_choice',\n",
    "                          'activity_pattern', 'imf_choice', 'inmf_choice', 'fp_choice', \n",
    "                          'reimb_pct', 'workDCLogsum', 'schoolDCLogsum'],\n",
    "        'file_type': 'person'\n",
    "    },\n",
    "    \n",
    "    'wsLocResults': {\n",
    "        'description': 'Work and school location choice results',\n",
    "        'expected_fields': ['hh_id', 'person_id', 'EmploymentCategory', 'StudentCategory', \n",
    "                          'WorkSegment', 'SchoolSegment', 'WorkLocation', 'WorkLocationDistance',\n",
    "                          'WorkLocationLogsum', 'SchoolLocation', 'SchoolLocationDistance', 'SchoolLocationLogsum'],\n",
    "        'file_type': 'location_choice'\n",
    "    },\n",
    "    \n",
    "    'indivTourData': {\n",
    "        'description': 'Individual Tours - one row per individual tour',\n",
    "        'expected_fields': ['hh_id', 'person_id', 'person_num', 'person_type', 'tour_id', \n",
    "                          'tour_category', 'tour_purpose', 'orig_mgra', 'dest_mgra', \n",
    "                          'start_period', 'end_period', 'tour_mode', 'tour_distance', \n",
    "                          'tour_time', 'atWork_freq', 'num_ob_stops', 'num_ib_stops',\n",
    "                          'out_btap', 'out_atap', 'in_btap', 'in_atap', 'out_set', 'in_set',\n",
    "                          'sampleRate', 'dcLogsum'] + [f'util_{i}' for i in range(1,18)] + [f'prob_{i}' for i in range(1,18)],\n",
    "        'file_type': 'individual_tour'\n",
    "    },\n",
    "    \n",
    "    'indivTripData': {\n",
    "        'description': 'Individual Trips - one row per individual trip',\n",
    "        'expected_fields': ['hh_id', 'person_id', 'person_num', 'tour_id', 'stop_id', 'inbound',\n",
    "                          'tour_purpose', 'orig_purpose', 'dest_purpose', 'orig_mgra', 'dest_mgra',\n",
    "                          'parking_mgra', 'stop_period', 'trip_mode', 'trip_board_tap', \n",
    "                          'trip_alight_tap', 'tour_mode', 'set', 'sampleRate', 'TRIP_TIME', \n",
    "                          'TRIP_DISTANCE', 'TRIP_COST'],\n",
    "        'file_type': 'individual_trip'\n",
    "    },\n",
    "    \n",
    "    'jointTourData': {\n",
    "        'description': 'Joint Tours - one row per joint tour (includes all travelers)',\n",
    "        'expected_fields': ['hh_id', 'tour_id', 'tour_category', 'tour_purpose', 'tour_composition',\n",
    "                          'tour_participants', 'orig_mgra', 'dest_mgra', 'start_period', 'end_period',\n",
    "                          'tour_mode', 'tour_distance', 'tour_time', 'num_ob_stops', 'num_ib_stops',\n",
    "                          'out_btap', 'out_atap', 'in_btap', 'in_atap', 'out_set', 'in_set',\n",
    "                          'sampleRate', 'dcLogsum'] + [f'util_{i}' for i in range(1,18)] + [f'prob_{i}' for i in range(1,18)],\n",
    "        'file_type': 'joint_tour'\n",
    "    },\n",
    "    \n",
    "    'jointTripData': {\n",
    "        'description': 'Joint Trips - one row per joint trip',\n",
    "        'expected_fields': ['hh_id', 'tour_id', 'stop_id', 'inbound', 'tour_purpose', 'orig_purpose',\n",
    "                          'dest_purpose', 'orig_mgra', 'dest_mgra', 'parking_mgra', 'stop_period',\n",
    "                          'trip_mode', 'num_participants', 'trip_board_tap', 'trip_alight_tap',\n",
    "                          'tour_mode', 'set', 'sampleRate', 'TRIP_TIME', 'TRIP_DISTANCE', 'TRIP_COST'],\n",
    "        'file_type': 'joint_trip'\n",
    "    },\n",
    "    \n",
    "    'indivTripDataResim': {\n",
    "        'description': 'Resimulated Transit Trips',\n",
    "        'expected_fields': ['hh_id', 'person_id', 'person_num', 'tour_id', 'stop_id', 'inbound',\n",
    "                          'tour_purpose', 'orig_purpose', 'dest_purpose', 'orig_mgra', 'dest_mgra',\n",
    "                          'parking_mgra', 'stop_period', 'trip_mode', 'trip_board_tap', \n",
    "                          'trip_alight_tap', 'tour_mode', 'set', 'sampleRate', 'resimulatedTrip',\n",
    "                          'TRIP_TIME', 'TRIP_DISTANCE', 'TRIP_COST'],\n",
    "        'file_type': 'resimulated_trip'\n",
    "    },\n",
    "    \n",
    "    'unconstrainedPNRDemand': {\n",
    "        'description': 'Unconstrained Parking Demand by TAP and time period',\n",
    "        'expected_fields': ['TAP', 'CAPACITY', 'TOT_ARRIVALS'],  # Plus time period columns\n",
    "        'file_type': 'parking_demand'\n",
    "    },\n",
    "    \n",
    "    'constrainedPNRDemand': {\n",
    "        'description': 'Constrained Parking Demand by TAP and time period', \n",
    "        'expected_fields': ['TAP', 'CAPACITY', 'TOT_ARRIVALS'],  # Plus time period columns\n",
    "        'file_type': 'parking_demand'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Mode codes mapping\n",
    "MODE_CODES = {\n",
    "    1: 'DRIVEALONEFREE', 2: 'DRIVEALONEPAY', 3: 'SHARED2GP', 4: 'SHARED2HOV', \n",
    "    5: 'SHARED2PAY', 6: 'SHARED3GP', 7: 'SHARED3HOV', 8: 'SHARED3PAY',\n",
    "    9: 'WALK', 10: 'BIKE', 11: 'WALK_SET', 12: 'PNR_SET', 13: 'KNR_PERS',\n",
    "    14: 'KNR_TNC', 15: 'TAXI', 16: 'TNC', 17: 'SCHBUS'\n",
    "}\n",
    "\n",
    "# Time period mapping\n",
    "TIME_PERIODS = {\n",
    "    1: '03:00 AM to 05:00 AM', 2: '05:00 AM to 05:30 AM', 3: '05:30 AM to 06:00 AM',\n",
    "    4: '06:00 AM to 06:30 AM', 5: '06:30 AM to 07:00 AM', 6: '07:00 AM to 07:30 AM',\n",
    "    7: '07:30 AM to 08:00 AM', 8: '08:00 AM to 08:30 AM', 9: '08:30 AM to 09:00 AM',\n",
    "    10: '09:00 AM to 09:30 AM', 11: '09:30 AM to 10:00 AM', 12: '10:00 AM to 10:30 AM',\n",
    "    13: '10:30 AM to 11:00 AM', 14: '11:00 AM to 11:30 AM', 15: '11:30 AM to 12:00 PM',\n",
    "    16: '12:00 PM to 12:30 PM', 17: '12:30 PM to 01:00 PM', 18: '01:00 PM to 01:30 PM',\n",
    "    19: '01:30 PM to 02:00 PM', 20: '02:00 PM to 02:30 PM', 21: '02:30 PM to 03:00 PM',\n",
    "    22: '03:00 PM to 03:30 PM', 23: '03:30 PM to 04:00 PM', 24: '04:00 PM to 04:30 PM',\n",
    "    25: '04:30 PM to 05:00 PM', 26: '05:00 PM to 05:30 PM', 27: '05:30 PM to 06:00 PM',\n",
    "    28: '06:00 PM to 06:30 PM', 29: '06:30 PM to 07:00 PM', 30: '07:00 PM to 07:30 PM',\n",
    "    31: '07:30 PM to 08:00 PM', 32: '08:00 PM to 08:30 PM', 33: '08:30 PM to 09:00 PM',\n",
    "    34: '09:00 PM to 09:30 PM', 35: '09:30 PM to 10:00 PM', 36: '10:00 PM to 10:30 PM',\n",
    "    37: '10:30 PM to 11:00 PM', 38: '11:00 PM to 11:30 PM', 39: '11:30 PM to 12:00 AM',\n",
    "    40: '12:00 AM to 03:00 AM'\n",
    "}\n",
    "\n",
    "print(\"‚úÖ CTRAMP documentation schemas loaded!\")\n",
    "print(f\"üìã Defined schemas for {len(CTRAMP_FILE_SCHEMAS)} file types\")\n",
    "print(f\"üöó Mode codes: {len(MODE_CODES)} modes defined\")  \n",
    "print(f\"‚è∞ Time periods: {len(TIME_PERIODS)} periods defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2196c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced CTRAMP Documentation for Analysis and Summarization\n",
    "# This comprehensive documentation will help with future analysis tasks\n",
    "\n",
    "ENHANCED_CTRAMP_SCHEMAS = {\n",
    "    'accessibilities.csv': {\n",
    "        'description': 'Zone-level accessibility measures by microzone (MGRA)',\n",
    "        'primary_key': ['mgra'],\n",
    "        'expected_fields': ['mgra'],  # Will be updated based on actual data\n",
    "        'file_type': 'accessibility',\n",
    "        'record_level': 'zone',\n",
    "        'typical_size': '10K-50K rows (one per MGRA)',\n",
    "        'analysis_notes': {\n",
    "            'purpose': 'Accessibility to jobs, schools, other activities by mode and time period',\n",
    "            'common_summaries': ['accessibility by county', 'accessibility by income', 'mode-specific accessibility'],\n",
    "            'key_fields_for_analysis': ['mgra', 'auto_accessibility', 'transit_accessibility'],\n",
    "            'geographic_aggregation': 'Can aggregate to TAZ, county, or equity zones using MGRA lookups'\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'aoResults_pre.csv': {\n",
    "        'description': 'Household auto ownership before work/school location choice',\n",
    "        'primary_key': ['hh_id'],\n",
    "        'expected_fields': ['hh_id'],  # Will be updated\n",
    "        'file_type': 'auto_ownership',\n",
    "        'record_level': 'household', \n",
    "        'typical_size': '500K-1M rows (one per household)',\n",
    "        'analysis_notes': {\n",
    "            'purpose': 'Pre-location choice vehicle ownership model results',\n",
    "            'common_summaries': ['vehicles per household by income', 'ownership rates by area type'],\n",
    "            'key_fields_for_analysis': ['hh_id', 'vehicles', 'income_category'],\n",
    "            'comparison_file': 'aoResults.csv shows post-location choice results'\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'aoResults.csv': {\n",
    "        'description': 'Household auto ownership after work/school location choice',\n",
    "        'primary_key': ['hh_id'],\n",
    "        'expected_fields': ['hh_id'],  # Will be updated\n",
    "        'file_type': 'auto_ownership',\n",
    "        'record_level': 'household',\n",
    "        'typical_size': '500K-1M rows (one per household)', \n",
    "        'analysis_notes': {\n",
    "            'purpose': 'Final vehicle ownership after location choices',\n",
    "            'common_summaries': ['change in ownership due to location', 'final ownership distribution'],\n",
    "            'key_fields_for_analysis': ['hh_id', 'vehicles', 'change_from_pre'],\n",
    "            'comparison_file': 'Compare with aoResults_pre.csv to see location impact'\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'householdData': {\n",
    "        'description': 'Household characteristics and choice model results',\n",
    "        'primary_key': ['hh_id'],\n",
    "        'expected_fields': ['hh_id', 'transponder', 'cdap_pattern', 'jtf_choice'],  # Will expand\n",
    "        'file_type': 'household',\n",
    "        'record_level': 'household',\n",
    "        'typical_size': '500K-1M rows (one per household)',\n",
    "        'analysis_notes': {\n",
    "            'purpose': 'Core household data with demographics and major choice model results',\n",
    "            'common_summaries': ['household size distribution', 'income distribution', 'CDAP patterns', 'joint tour frequency'],\n",
    "            'key_fields_for_analysis': ['hh_id', 'hhsize', 'income', 'workers', 'cdap_pattern', 'vehicles'],\n",
    "            'geographic_aggregation': 'Use home_mgra to aggregate spatially',\n",
    "            'model_validation': 'Compare CDAP patterns to survey data, check income distribution'\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'personData': {\n",
    "        'description': 'Person characteristics and individual choice model results',\n",
    "        'primary_key': ['hh_id', 'person_id'],\n",
    "        'expected_fields': ['hh_id', 'person_id', 'person_num', 'value_of_time', \n",
    "                          'transitSubsidy_choice', 'transitSubsidy_percent', 'transitPass_choice',\n",
    "                          'activity_pattern', 'imf_choice', 'inmf_choice', 'fp_choice', \n",
    "                          'reimb_pct', 'workDCLogsum', 'schoolDCLogsum'],\n",
    "        'file_type': 'person',\n",
    "        'record_level': 'person',\n",
    "        'typical_size': '1M-2M rows (2-3 persons per household average)',\n",
    "        'analysis_notes': {\n",
    "            'purpose': 'Individual person attributes and activity/tour generation results',\n",
    "            'common_summaries': ['person type distribution', 'activity patterns by demographics', 'value of time distribution'],\n",
    "            'key_fields_for_analysis': ['person_type', 'age', 'gender', 'worker_status', 'activity_pattern', 'value_of_time'],\n",
    "            'model_validation': 'Check activity pattern rates vs survey, validate value of time by income',\n",
    "            'join_keys': 'Join to householdData on hh_id for household characteristics'\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'wsLocResults': {\n",
    "        'description': 'Work and school location choice model results',\n",
    "        'primary_key': ['hh_id', 'person_id'],\n",
    "        'expected_fields': ['hh_id', 'person_id', 'EmploymentCategory', 'StudentCategory', \n",
    "                          'WorkSegment', 'SchoolSegment', 'WorkLocation', 'WorkLocationDistance',\n",
    "                          'WorkLocationLogsum', 'SchoolLocation', 'SchoolLocationDistance', 'SchoolLocationLogsum'],\n",
    "        'file_type': 'location_choice',\n",
    "        'record_level': 'person',\n",
    "        'typical_size': '500K-1M rows (workers and students only)',\n",
    "        'analysis_notes': {\n",
    "            'purpose': 'Workplace and school location choices with distances and logsums',\n",
    "            'common_summaries': ['commute distance distribution', 'jobs-housing balance', 'school enrollment by district'],\n",
    "            'key_fields_for_analysis': ['WorkLocation', 'WorkLocationDistance', 'SchoolLocation', 'employment_category'],\n",
    "            'geographic_analysis': 'Map work/school flows, calculate commute patterns by county',\n",
    "            'model_validation': 'Compare commute distances to survey data'\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'indivTourData': {\n",
    "        'description': 'Individual tours with full mode choice model results',\n",
    "        'primary_key': ['hh_id', 'person_id', 'tour_id'],\n",
    "        'expected_fields': ['hh_id', 'person_id', 'person_num', 'person_type', 'tour_id', \n",
    "                          'tour_category', 'tour_purpose', 'orig_mgra', 'dest_mgra', \n",
    "                          'start_period', 'end_period', 'tour_mode', 'tour_distance', \n",
    "                          'tour_time', 'atWork_freq', 'num_ob_stops', 'num_ib_stops',\n",
    "                          'out_btap', 'out_atap', 'in_btap', 'in_atap', 'out_set', 'in_set',\n",
    "                          'sampleRate', 'dcLogsum'] + [f'util_{i}' for i in range(1,18)] + [f'prob_{i}' for i in range(1,18)],\n",
    "        'file_type': 'individual_tour',\n",
    "        'record_level': 'tour',\n",
    "        'typical_size': '2M-5M rows (2-3 tours per person average)',\n",
    "        'analysis_notes': {\n",
    "            'purpose': 'Individual daily tours with mode choice utilities and probabilities',\n",
    "            'common_summaries': ['mode share by purpose', 'tour generation rates', 'time-of-day patterns', 'tour length distribution'],\n",
    "            'key_fields_for_analysis': ['tour_purpose', 'tour_mode', 'start_period', 'end_period', 'tour_distance', 'orig_mgra', 'dest_mgra'],\n",
    "            'mode_choice_analysis': 'Use util_* and prob_* fields for mode choice model validation',\n",
    "            'time_analysis': 'start_period and end_period for departure/arrival time analysis',\n",
    "            'geographic_analysis': 'Origin-destination flows using orig_mgra and dest_mgra',\n",
    "            'transit_analysis': 'Use *_btap and *_atap fields for transit boarding/alighting analysis'\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'indivTripData': {\n",
    "        'description': 'Individual trips (tour components) with detailed attributes',\n",
    "        'primary_key': ['hh_id', 'person_id', 'tour_id', 'stop_id'],\n",
    "        'expected_fields': ['hh_id', 'person_id', 'person_num', 'tour_id', 'stop_id', 'inbound',\n",
    "                          'tour_purpose', 'orig_purpose', 'dest_purpose', 'orig_mgra', 'dest_mgra',\n",
    "                          'parking_mgra', 'stop_period', 'trip_mode', 'trip_board_tap', \n",
    "                          'trip_alight_tap', 'tour_mode', 'set', 'sampleRate', 'TRIP_TIME', \n",
    "                          'TRIP_DISTANCE', 'TRIP_COST'],\n",
    "        'file_type': 'individual_trip',\n",
    "        'record_level': 'trip',\n",
    "        'typical_size': '5M-15M rows (multiple trips per tour)',\n",
    "        'analysis_notes': {\n",
    "            'purpose': 'Individual trip segments with time, distance, and cost',\n",
    "            'common_summaries': ['trip mode share', 'trip length distribution', 'VMT by purpose', 'trip timing patterns'],\n",
    "            'key_fields_for_analysis': ['trip_mode', 'orig_purpose', 'dest_purpose', 'TRIP_DISTANCE', 'TRIP_TIME', 'stop_period'],\n",
    "            'vmt_calculation': 'Sum TRIP_DISTANCE by trip_mode for vehicle miles traveled',\n",
    "            'transit_analysis': 'trip_board_tap and trip_alight_tap for detailed transit flows',\n",
    "            'time_analysis': 'stop_period for hourly travel patterns',\n",
    "            'cost_analysis': 'TRIP_COST for travel cost distribution by income/mode'\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'jointTourData': {\n",
    "        'description': 'Joint household tours (multiple participants)',\n",
    "        'primary_key': ['hh_id', 'tour_id'],\n",
    "        'expected_fields': ['hh_id', 'tour_id', 'tour_category', 'tour_purpose', 'tour_composition',\n",
    "                          'tour_participants', 'orig_mgra', 'dest_mgra', 'start_period', 'end_period',\n",
    "                          'tour_mode', 'tour_distance', 'tour_time', 'num_ob_stops', 'num_ib_stops',\n",
    "                          'out_btap', 'out_atap', 'in_btap', 'in_atap', 'out_set', 'in_set',\n",
    "                          'sampleRate', 'dcLogsum'] + [f'util_{i}' for i in range(1,18)] + [f'prob_{i}' for i in range(1,18)],\n",
    "        'file_type': 'joint_tour',\n",
    "        'record_level': 'joint_tour',\n",
    "        'typical_size': '500K-2M rows (fewer than individual tours)',\n",
    "        'analysis_notes': {\n",
    "            'purpose': 'Household joint travel with multiple participants per tour',\n",
    "            'common_summaries': ['joint travel rates', 'joint tour purposes', 'household coordination patterns'],\n",
    "            'key_fields_for_analysis': ['tour_purpose', 'tour_composition', 'tour_participants', 'tour_mode'],\n",
    "            'occupancy_analysis': 'Use tour_participants to understand vehicle occupancy',\n",
    "            'household_analysis': 'Compare joint vs individual travel patterns by household type'\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'jointTripData': {\n",
    "        'description': 'Joint household trips (components of joint tours)',  \n",
    "        'primary_key': ['hh_id', 'tour_id', 'stop_id'],\n",
    "        'expected_fields': ['hh_id', 'tour_id', 'stop_id', 'inbound', 'tour_purpose', 'orig_purpose',\n",
    "                          'dest_purpose', 'orig_mgra', 'dest_mgra', 'parking_mgra', 'stop_period',\n",
    "                          'trip_mode', 'num_participants', 'trip_board_tap', 'trip_alight_tap',\n",
    "                          'tour_mode', 'set', 'sampleRate', 'TRIP_TIME', 'TRIP_DISTANCE', 'TRIP_COST'],\n",
    "        'file_type': 'joint_trip',\n",
    "        'record_level': 'joint_trip',\n",
    "        'typical_size': '1M-5M rows (trips within joint tours)',\n",
    "        'analysis_notes': {\n",
    "            'purpose': 'Joint trip segments with participant counts',\n",
    "            'common_summaries': ['joint trip VMT', 'vehicle occupancy rates', 'joint trip destinations'],\n",
    "            'key_fields_for_analysis': ['trip_mode', 'num_participants', 'TRIP_DISTANCE', 'orig_purpose', 'dest_purpose'],\n",
    "            'occupancy_calculation': 'Use num_participants for accurate vehicle occupancy analysis',\n",
    "            'vmt_adjustment': 'TRIP_DISTANCE represents vehicle miles, not person miles'\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'indivTripDataResim': {\n",
    "        'description': 'Resimulated transit trips with additional transit assignment data',\n",
    "        'primary_key': ['hh_id', 'person_id', 'tour_id', 'stop_id'],\n",
    "        'expected_fields': ['hh_id', 'person_id', 'person_num', 'tour_id', 'stop_id', 'inbound',\n",
    "                          'tour_purpose', 'orig_purpose', 'dest_purpose', 'orig_mgra', 'dest_mgra',\n",
    "                          'parking_mgra', 'stop_period', 'trip_mode', 'trip_board_tap', \n",
    "                          'trip_alight_tap', 'tour_mode', 'set', 'sampleRate', 'resimulatedTrip',\n",
    "                          'TRIP_TIME', 'TRIP_DISTANCE', 'TRIP_COST'],\n",
    "        'file_type': 'resimulated_trip',\n",
    "        'record_level': 'trip',\n",
    "        'typical_size': 'Subset of indivTripData (transit trips only)',\n",
    "        'analysis_notes': {\n",
    "            'purpose': 'Transit trips with updated paths from transit assignment feedback',\n",
    "            'common_summaries': ['transit mode share', 'transit boarding patterns', 'route choice validation'],\n",
    "            'key_fields_for_analysis': ['resimulatedTrip', 'trip_board_tap', 'trip_alight_tap', 'set'],\n",
    "            'transit_validation': 'Compare with original indivTripData to see assignment impacts',\n",
    "            'path_analysis': 'Use set field to analyze transit route choices'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Enhanced CTRAMP documentation loaded for analysis!\")\n",
    "print(f\"üìä Detailed schemas with analysis guidance for {len(ENHANCED_CTRAMP_SCHEMAS)} file types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54198ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis Helper Functions for Future Summarization Tasks\n",
    "\n",
    "def get_analysis_guidance(file_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Get specific analysis guidance for a CTRAMP file type\n",
    "    \"\"\"\n",
    "    for pattern, schema in ENHANCED_CTRAMP_SCHEMAS.items():\n",
    "        if pattern.replace('_[iteration]', '').replace('[iteration]', '') in file_type.lower():\n",
    "            return schema.get('analysis_notes', {})\n",
    "    return {}\n",
    "\n",
    "def get_common_summaries(filename: str) -> list:\n",
    "    \"\"\"\n",
    "    Get list of common summary analyses for a file type\n",
    "    \"\"\"\n",
    "    guidance = get_analysis_guidance(filename)\n",
    "    return guidance.get('common_summaries', [])\n",
    "\n",
    "def get_key_analysis_fields(filename: str) -> list:\n",
    "    \"\"\"\n",
    "    Get the most important fields for analysis in a file\n",
    "    \"\"\"\n",
    "    guidance = get_analysis_guidance(filename)\n",
    "    return guidance.get('key_fields_for_analysis', [])\n",
    "\n",
    "def estimate_expected_size(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Get expected file size/record count for validation\n",
    "    \"\"\"\n",
    "    for pattern, schema in ENHANCED_CTRAMP_SCHEMAS.items():\n",
    "        if pattern.replace('_[iteration]', '').replace('[iteration]', '') in filename.lower():\n",
    "            return schema.get('typical_size', 'Size varies')\n",
    "    return 'Unknown size'\n",
    "\n",
    "# Summary Analysis Templates\n",
    "SUMMARY_TEMPLATES = {\n",
    "    'mode_share': {\n",
    "        'description': 'Calculate mode share by various demographics and geography',\n",
    "        'applicable_files': ['indivTourData', 'indivTripData', 'jointTourData', 'jointTripData'],\n",
    "        'key_fields': ['tour_mode', 'trip_mode'],\n",
    "        'groupby_options': ['tour_purpose', 'income_category', 'person_type', 'time_period'],\n",
    "        'code_template': \"\"\"\n",
    "# Mode Share Analysis\n",
    "mode_share = df.groupby(['groupby_field'])['mode_field'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "mode_share_pct = mode_share * 100\n",
    "        \"\"\"\n",
    "    },\n",
    "    \n",
    "    'vmt_calculation': {\n",
    "        'description': 'Calculate Vehicle Miles Traveled by various categories',\n",
    "        'applicable_files': ['indivTripData', 'jointTripData'],\n",
    "        'key_fields': ['TRIP_DISTANCE', 'trip_mode'],\n",
    "        'vehicle_modes': [1, 2, 3, 4, 5, 6, 7, 8],  # Drive alone and shared ride modes\n",
    "        'code_template': \"\"\"\n",
    "# VMT Calculation\n",
    "vehicle_trips = df[df['trip_mode'].isin([1,2,3,4,5,6,7,8])]\n",
    "vmt_by_purpose = vehicle_trips.groupby('orig_purpose')['TRIP_DISTANCE'].sum()\n",
    "        \"\"\"\n",
    "    },\n",
    "    \n",
    "    'time_of_day': {\n",
    "        'description': 'Analyze departure/arrival time patterns',\n",
    "        'applicable_files': ['indivTourData', 'indivTripData'],\n",
    "        'key_fields': ['start_period', 'end_period', 'stop_period'],\n",
    "        'time_periods': TIME_PERIODS,\n",
    "        'code_template': \"\"\"\n",
    "# Time of Day Analysis\n",
    "tod_pattern = df.groupby(['time_field']).size()\n",
    "# Convert period numbers to time labels using TIME_PERIODS dict\n",
    "        \"\"\"\n",
    "    },\n",
    "    \n",
    "    'accessibility_summary': {\n",
    "        'description': 'Summarize accessibility measures by geography and demographics',\n",
    "        'applicable_files': ['accessibilities.csv'],\n",
    "        'key_fields': ['mgra'],\n",
    "        'join_requirements': 'Need MGRA to TAZ/County lookup table',\n",
    "        'code_template': \"\"\"\n",
    "# Accessibility Summary by Geography\n",
    "# Join with geography lookup: accessibility.merge(mgra_lookup, on='mgra')\n",
    "acc_by_county = df.groupby('county')[accessibility_fields].mean()\n",
    "        \"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_summary_template(analysis_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Get a pre-built analysis template\n",
    "    \"\"\"\n",
    "    return SUMMARY_TEMPLATES.get(analysis_type, {})\n",
    "\n",
    "def list_applicable_summaries(filename: str) -> list:\n",
    "    \"\"\"\n",
    "    List all applicable summary analyses for a given file\n",
    "    \"\"\"\n",
    "    applicable = []\n",
    "    for analysis_type, template in SUMMARY_TEMPLATES.items():\n",
    "        if any(file_pattern in filename for file_pattern in template['applicable_files']):\n",
    "            applicable.append(analysis_type)\n",
    "    return applicable\n",
    "\n",
    "print(\"üõ†Ô∏è Analysis helper functions loaded!\")\n",
    "print(f\"üìù {len(SUMMARY_TEMPLATES)} pre-built analysis templates available:\")\n",
    "for template_name in SUMMARY_TEMPLATES.keys():\n",
    "    print(f\"   ‚Ä¢ {template_name}\")\n",
    "\n",
    "# Quick reference for mode codes and time periods\n",
    "print(f\"\\nüöó Mode codes: {len(MODE_CODES)} modes (1=Drive Alone Free, 9=Walk, 11=Walk-Transit, etc.)\")\n",
    "print(f\"‚è∞ Time periods: {len(TIME_PERIODS)} periods (1=3-5am, 8=8-8:30am peak, etc.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f7c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update validation functions to use enhanced documentation\n",
    "def validate_with_enhanced_schemas(file_info: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Enhanced validation using the detailed analysis-ready schemas\n",
    "    \"\"\"\n",
    "    filename = file_info['filename']\n",
    "    \n",
    "    # Find matching enhanced schema\n",
    "    enhanced_schema = None\n",
    "    schema_key = None\n",
    "    for pattern, schema in ENHANCED_CTRAMP_SCHEMAS.items():\n",
    "        if pattern.replace('_[iteration]', '').replace('[iteration]', '') in filename.lower():\n",
    "            enhanced_schema = schema\n",
    "            schema_key = pattern\n",
    "            break\n",
    "    \n",
    "    validation_result = {\n",
    "        'filename': filename,\n",
    "        'identified_type': schema_key,\n",
    "        'record_level': enhanced_schema.get('record_level', 'unknown') if enhanced_schema else 'unknown',\n",
    "        'expected_size': enhanced_schema.get('typical_size', 'unknown') if enhanced_schema else 'unknown',\n",
    "        'primary_keys': enhanced_schema.get('primary_key', []) if enhanced_schema else [],\n",
    "        'analysis_ready': enhanced_schema is not None,\n",
    "        'common_analyses': enhanced_schema.get('analysis_notes', {}).get('common_summaries', []) if enhanced_schema else [],\n",
    "        'key_analysis_fields': enhanced_schema.get('analysis_notes', {}).get('key_fields_for_analysis', []) if enhanced_schema else [],\n",
    "        'applicable_templates': list_applicable_summaries(filename),\n",
    "        'field_coverage': 0,\n",
    "        'missing_critical_fields': [],\n",
    "        'validation_notes': []\n",
    "    }\n",
    "    \n",
    "    if file_info.get('error'):\n",
    "        validation_result['validation_notes'].append(f\"ERROR: {file_info['error']}\")\n",
    "        return validation_result\n",
    "    \n",
    "    actual_columns = set(file_info.get('columns', []))\n",
    "    \n",
    "    if enhanced_schema:\n",
    "        expected_fields = set(enhanced_schema['expected_fields'])\n",
    "        critical_fields = set(enhanced_schema.get('primary_key', []))\n",
    "        analysis_fields = set(enhanced_schema.get('analysis_notes', {}).get('key_fields_for_analysis', []))\n",
    "        \n",
    "        # Calculate field coverage\n",
    "        matching_expected = len(expected_fields & actual_columns)\n",
    "        if len(expected_fields) > 0:\n",
    "            validation_result['field_coverage'] = (matching_expected / len(expected_fields)) * 100\n",
    "        \n",
    "        # Check for missing critical fields\n",
    "        validation_result['missing_critical_fields'] = list(critical_fields - actual_columns)\n",
    "        missing_analysis_fields = list(analysis_fields - actual_columns)\n",
    "        \n",
    "        # Validation notes\n",
    "        if validation_result['field_coverage'] >= 90:\n",
    "            validation_result['validation_notes'].append(\"‚úÖ Excellent field coverage\")\n",
    "        elif validation_result['field_coverage'] >= 70:\n",
    "            validation_result['validation_notes'].append(\"üëç Good field coverage\")\n",
    "        else:\n",
    "            validation_result['validation_notes'].append(\"‚ö†Ô∏è Low field coverage - check schema\")\n",
    "            \n",
    "        if validation_result['missing_critical_fields']:\n",
    "            validation_result['validation_notes'].append(f\"‚ùå Missing primary keys: {validation_result['missing_critical_fields']}\")\n",
    "            \n",
    "        if missing_analysis_fields:\n",
    "            validation_result['validation_notes'].append(f\"üìä Missing key analysis fields: {missing_analysis_fields[:3]}\")\n",
    "            \n",
    "        # Size validation\n",
    "        if file_info.get('shape'):\n",
    "            actual_rows = file_info['shape'][0]\n",
    "            expected_size = enhanced_schema.get('typical_size', '')\n",
    "            if 'K' in expected_size:\n",
    "                min_expected = int(expected_size.split('-')[0].replace('K', '')) * 1000\n",
    "                if actual_rows < min_expected * 0.5:  # Allow 50% tolerance\n",
    "                    validation_result['validation_notes'].append(f\"‚ö†Ô∏è Low row count: {actual_rows:,} vs expected {expected_size}\")\n",
    "    else:\n",
    "        validation_result['validation_notes'].append(\"‚ùì File type not in enhanced documentation\")\n",
    "    \n",
    "    return validation_result\n",
    "\n",
    "# Function to show analysis recommendations for a file\n",
    "def show_analysis_recommendations(filename: str, file_columns: list = None):\n",
    "    \"\"\"\n",
    "    Show specific analysis recommendations for a file\n",
    "    \"\"\"\n",
    "    print(f\"\\nüéØ ANALYSIS RECOMMENDATIONS FOR: {filename}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get guidance\n",
    "    guidance = get_analysis_guidance(filename)\n",
    "    if not guidance:\n",
    "        print(\"‚ùì No specific guidance available for this file type\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìù Purpose: {guidance.get('purpose', 'No description')}\")\n",
    "    \n",
    "    print(f\"\\nüîç Common Summaries:\")\n",
    "    for summary in guidance.get('common_summaries', []):\n",
    "        print(f\"   ‚Ä¢ {summary}\")\n",
    "    \n",
    "    print(f\"\\nüìä Key Fields for Analysis:\")\n",
    "    key_fields = guidance.get('key_fields_for_analysis', [])\n",
    "    for field in key_fields:\n",
    "        available = \"‚úÖ\" if file_columns and field in file_columns else \"‚ùì\"\n",
    "        print(f\"   {available} {field}\")\n",
    "    \n",
    "    print(f\"\\nüõ†Ô∏è Applicable Analysis Templates:\")\n",
    "    templates = list_applicable_summaries(filename)\n",
    "    for template in templates:\n",
    "        template_info = SUMMARY_TEMPLATES[template]\n",
    "        print(f\"   ‚Ä¢ {template}: {template_info['description']}\")\n",
    "    \n",
    "    # Show any special analysis notes\n",
    "    for note_type in ['geographic_aggregation', 'model_validation', 'join_keys', 'vmt_calculation']:\n",
    "        if note_type in guidance:\n",
    "            print(f\"\\nüí° {note_type.replace('_', ' ').title()}: {guidance[note_type]}\")\n",
    "\n",
    "print(\"üéØ Enhanced validation and analysis recommendation functions loaded!\")\n",
    "print(\"üìö This documentation will help with:\")\n",
    "print(\"   ‚Ä¢ Field validation against expected schemas\")\n",
    "print(\"   ‚Ä¢ Analysis recommendations for each file type\") \n",
    "print(\"   ‚Ä¢ Pre-built analysis templates\")\n",
    "print(\"   ‚Ä¢ Data quality validation\")\n",
    "print(\"   ‚Ä¢ Size and structure expectations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e9f4358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Validating discovered files against CTRAMP documentation...\n",
      "‚úÖ Validated 12 files\n"
     ]
    }
   ],
   "source": [
    "def identify_ctramp_file_type(filename: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Identify the CTRAMP file type based on filename patterns\n",
    "    Returns: (file_type_key, expected_schema, description)\n",
    "    \"\"\"\n",
    "    filename_lower = filename.lower()\n",
    "    \n",
    "    # Check each known pattern\n",
    "    for pattern, schema in CTRAMP_FILE_SCHEMAS.items():\n",
    "        if pattern.replace('_[iteration]', '').replace('[iteration]', '') in filename_lower:\n",
    "            return pattern, schema, schema['description']\n",
    "    \n",
    "    # Check for specific patterns with iterations\n",
    "    for base_name in ['householdData', 'personData', 'wsLocResults', 'indivTourData', \n",
    "                      'indivTripData', 'jointTourData', 'jointTripData', 'indivTripDataResim',\n",
    "                      'unconstrainedPNRDemand', 'constrainedPNRDemand']:\n",
    "        if base_name.lower() in filename_lower:\n",
    "            return base_name, CTRAMP_FILE_SCHEMAS[base_name], CTRAMP_FILE_SCHEMAS[base_name]['description']\n",
    "    \n",
    "    return None, None, \"Unknown CTRAMP file type\"\n",
    "\n",
    "def validate_schema_against_documentation(file_info: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Validate discovered schema against expected CTRAMP documentation\n",
    "    \"\"\"\n",
    "    filename = file_info['filename']\n",
    "    file_type_key, expected_schema, description = identify_ctramp_file_type(filename)\n",
    "    \n",
    "    validation_result = {\n",
    "        'filename': filename,\n",
    "        'identified_type': file_type_key,\n",
    "        'description': description,\n",
    "        'validation_status': 'unknown',\n",
    "        'missing_fields': [],\n",
    "        'unexpected_fields': [],\n",
    "        'field_match_percentage': 0,\n",
    "        'total_expected_fields': 0,\n",
    "        'total_actual_fields': len(file_info.get('columns', [])),\n",
    "        'has_utility_fields': False,\n",
    "        'has_probability_fields': False,\n",
    "        'appears_to_be_ctramp': False\n",
    "    }\n",
    "    \n",
    "    if file_info.get('error'):\n",
    "        validation_result['validation_status'] = 'error'\n",
    "        return validation_result\n",
    "        \n",
    "    actual_columns = set(file_info.get('columns', []))\n",
    "    \n",
    "    if expected_schema:\n",
    "        expected_fields = set(expected_schema['expected_fields'])\n",
    "        validation_result['total_expected_fields'] = len(expected_fields)\n",
    "        validation_result['appears_to_be_ctramp'] = True\n",
    "        \n",
    "        # Find missing and unexpected fields\n",
    "        validation_result['missing_fields'] = list(expected_fields - actual_columns)\n",
    "        validation_result['unexpected_fields'] = list(actual_columns - expected_fields)\n",
    "        \n",
    "        # Calculate match percentage (only for core fields, utility/prob fields are dynamic)\n",
    "        core_expected = {f for f in expected_fields if not (f.startswith('util_') or f.startswith('prob_'))}\n",
    "        matching_fields = len(core_expected & actual_columns)\n",
    "        if len(core_expected) > 0:\n",
    "            validation_result['field_match_percentage'] = (matching_fields / len(core_expected)) * 100\n",
    "        \n",
    "        # Check for utility and probability fields (mode choice models)\n",
    "        validation_result['has_utility_fields'] = any(col.startswith('util_') for col in actual_columns)\n",
    "        validation_result['has_probability_fields'] = any(col.startswith('prob_') for col in actual_columns)\n",
    "        \n",
    "        # Determine overall validation status\n",
    "        if validation_result['field_match_percentage'] >= 80:\n",
    "            validation_result['validation_status'] = 'excellent'\n",
    "        elif validation_result['field_match_percentage'] >= 60:\n",
    "            validation_result['validation_status'] = 'good'\n",
    "        elif validation_result['field_match_percentage'] >= 40:\n",
    "            validation_result['validation_status'] = 'partial'\n",
    "        else:\n",
    "            validation_result['validation_status'] = 'poor'\n",
    "    else:\n",
    "        # Check if it looks like a CTRAMP file based on common fields\n",
    "        ctramp_indicators = {'hh_id', 'person_id', 'tour_id', 'trip_mode', 'tour_mode', 'mgra'}\n",
    "        if any(indicator in actual_columns for indicator in ctramp_indicators):\n",
    "            validation_result['appears_to_be_ctramp'] = True\n",
    "            validation_result['validation_status'] = 'unrecognized_ctramp'\n",
    "        else:\n",
    "            validation_result['validation_status'] = 'non_ctramp'\n",
    "            \n",
    "    return validation_result\n",
    "\n",
    "# Validate all discovered files against CTRAMP documentation\n",
    "print(\"üîç Validating discovered files against CTRAMP documentation...\")\n",
    "validation_results = []\n",
    "\n",
    "for file_info in schema_results.get('file_schemas', []):\n",
    "    validation = validate_schema_against_documentation(file_info)\n",
    "    validation_results.append(validation)\n",
    "\n",
    "print(f\"‚úÖ Validated {len(validation_results)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65f49386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä CTRAMP VALIDATION SUMMARY\n",
      "================================================================================\n",
      "üìÅ Total Files Analyzed: 12\n",
      "üéØ CTRAMP Files Identified: 12\n",
      "‚úÖ Excellent Matches (80%+): 4\n",
      "üëç Good Matches (60-80%): 8\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚úÖ FILE 1: householdData_1.csv\n",
      "   Type: householdData\n",
      "   Description: Household level model results\n",
      "   Status: Excellent\n",
      "   Field Match: 100.0% (12 actual vs 4 expected)\n",
      "   Extra Fields (8): income, autos, workers, automated_vehicles, size...\n",
      "------------------------------------------------------------\n",
      "\n",
      "üëç FILE 2: indivTourData_1.csv\n",
      "   Type: indivTourData\n",
      "   Description: Individual Tours - one row per individual tour\n",
      "   Status: Good\n",
      "   Field Match: 76.0% (54 actual vs 59 expected)\n",
      "   Missing Fields (6): out_set, in_btap, out_atap, out_btap, in_atap...\n",
      "   Extra Fields (1): avAvailable\n",
      "   ‚úì Has utility fields (mode choice model)\n",
      "   ‚úì Has probability fields (mode choice model)\n",
      "------------------------------------------------------------\n",
      "\n",
      "üëç FILE 3: indivTripData_1.csv\n",
      "   Type: indivTripData\n",
      "   Description: Individual Trips - one row per individual trip\n",
      "   Status: Good\n",
      "   Field Match: 72.7% (19 actual vs 22 expected)\n",
      "   Missing Fields (6): TRIP_TIME, trip_alight_tap, set, trip_board_tap, TRIP_DISTANCE...\n",
      "   Extra Fields (3): trip_dist, tranpath_rnum, avAvailable\n",
      "------------------------------------------------------------\n",
      "\n",
      "üëç FILE 4: jointTourData_1.csv\n",
      "   Type: jointTourData\n",
      "   Description: Joint Tours - one row per joint tour (includes all travelers)\n",
      "   Status: Good\n",
      "   Field Match: 73.9% (52 actual vs 57 expected)\n",
      "   Missing Fields (6): out_set, in_btap, out_atap, out_btap, in_atap...\n",
      "   Extra Fields (1): avAvailable\n",
      "   ‚úì Has utility fields (mode choice model)\n",
      "   ‚úì Has probability fields (mode choice model)\n",
      "------------------------------------------------------------\n",
      "\n",
      "üëç FILE 5: jointTripData_1.csv\n",
      "   Type: jointTripData\n",
      "   Description: Joint Trips - one row per joint trip\n",
      "   Status: Good\n",
      "   Field Match: 71.4% (18 actual vs 21 expected)\n",
      "   Missing Fields (6): TRIP_TIME, trip_alight_tap, set, trip_board_tap, TRIP_DISTANCE...\n",
      "   Extra Fields (3): tranpath_rnum, avAvailable, trip_dist\n",
      "------------------------------------------------------------\n",
      "\n",
      "‚úÖ FILE 6: personData_1.csv\n",
      "   Type: personData\n",
      "   Description: Person level model results\n",
      "   Status: Excellent\n",
      "   Field Match: 92.9% (21 actual vs 14 expected)\n",
      "   Missing Fields (1): activity_pattern\n",
      "   Extra Fields (8): sampleRate, naicsCode, cdap, type, telecommute...\n",
      "------------------------------------------------------------\n",
      "\n",
      "‚úÖ FILE 7: householdData_1.csv\n",
      "   Type: householdData\n",
      "   Description: Household level model results\n",
      "   Status: Excellent\n",
      "   Field Match: 100.0% (12 actual vs 4 expected)\n",
      "   Extra Fields (8): income, autos, workers, automated_vehicles, size...\n",
      "------------------------------------------------------------\n",
      "\n",
      "üëç FILE 8: indivTourData_1.csv\n",
      "   Type: indivTourData\n",
      "   Description: Individual Tours - one row per individual tour\n",
      "   Status: Good\n",
      "   Field Match: 76.0% (54 actual vs 59 expected)\n",
      "   Missing Fields (6): out_set, in_btap, out_atap, out_btap, in_atap...\n",
      "   Extra Fields (1): avAvailable\n",
      "   ‚úì Has utility fields (mode choice model)\n",
      "   ‚úì Has probability fields (mode choice model)\n",
      "------------------------------------------------------------\n",
      "\n",
      "üëç FILE 9: indivTripData_1.csv\n",
      "   Type: indivTripData\n",
      "   Description: Individual Trips - one row per individual trip\n",
      "   Status: Good\n",
      "   Field Match: 72.7% (19 actual vs 22 expected)\n",
      "   Missing Fields (6): TRIP_TIME, trip_alight_tap, set, trip_board_tap, TRIP_DISTANCE...\n",
      "   Extra Fields (3): trip_dist, tranpath_rnum, avAvailable\n",
      "------------------------------------------------------------\n",
      "\n",
      "üëç FILE 10: jointTourData_1.csv\n",
      "   Type: jointTourData\n",
      "   Description: Joint Tours - one row per joint tour (includes all travelers)\n",
      "   Status: Good\n",
      "   Field Match: 73.9% (52 actual vs 57 expected)\n",
      "   Missing Fields (6): out_set, in_btap, out_atap, out_btap, in_atap...\n",
      "   Extra Fields (1): avAvailable\n",
      "   ‚úì Has utility fields (mode choice model)\n",
      "   ‚úì Has probability fields (mode choice model)\n",
      "------------------------------------------------------------\n",
      "\n",
      "üëç FILE 11: jointTripData_1.csv\n",
      "   Type: jointTripData\n",
      "   Description: Joint Trips - one row per joint trip\n",
      "   Status: Good\n",
      "   Field Match: 71.4% (18 actual vs 21 expected)\n",
      "   Missing Fields (6): TRIP_TIME, trip_alight_tap, set, trip_board_tap, TRIP_DISTANCE...\n",
      "   Extra Fields (3): tranpath_rnum, avAvailable, trip_dist\n",
      "------------------------------------------------------------\n",
      "\n",
      "‚úÖ FILE 12: personData_1.csv\n",
      "   Type: personData\n",
      "   Description: Person level model results\n",
      "   Status: Excellent\n",
      "   Field Match: 92.9% (21 actual vs 14 expected)\n",
      "   Missing Fields (1): activity_pattern\n",
      "   Extra Fields (8): sampleRate, naicsCode, cdap, type, telecommute...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive validation report\n",
    "def print_validation_summary(validation_results: list):\n",
    "    \"\"\"\n",
    "    Print a comprehensive validation summary with CTRAMP documentation comparison\n",
    "    \"\"\"\n",
    "    print(\"üìä CTRAMP VALIDATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_files = len(validation_results)\n",
    "    ctramp_files = sum(1 for v in validation_results if v['appears_to_be_ctramp'])\n",
    "    excellent_matches = sum(1 for v in validation_results if v['validation_status'] == 'excellent')\n",
    "    good_matches = sum(1 for v in validation_results if v['validation_status'] == 'good')\n",
    "    \n",
    "    print(f\"üìÅ Total Files Analyzed: {total_files}\")\n",
    "    print(f\"üéØ CTRAMP Files Identified: {ctramp_files}\")\n",
    "    print(f\"‚úÖ Excellent Matches (80%+): {excellent_matches}\")\n",
    "    print(f\"üëç Good Matches (60-80%): {good_matches}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Detailed file-by-file analysis\n",
    "    for i, validation in enumerate(validation_results, 1):\n",
    "        status_emoji = {\n",
    "            'excellent': '‚úÖ', 'good': 'üëç', 'partial': '‚ö†Ô∏è', \n",
    "            'poor': '‚ùå', 'error': 'üí•', 'unrecognized_ctramp': '‚ùì', 'non_ctramp': 'üìÑ'\n",
    "        }\n",
    "        \n",
    "        emoji = status_emoji.get(validation['validation_status'], '‚ùì')\n",
    "        print(f\"\\n{emoji} FILE {i}: {validation['filename']}\")\n",
    "        print(f\"   Type: {validation['identified_type'] or 'Unknown'}\")\n",
    "        print(f\"   Description: {validation['description']}\")\n",
    "        print(f\"   Status: {validation['validation_status'].replace('_', ' ').title()}\")\n",
    "        \n",
    "        if validation['appears_to_be_ctramp']:\n",
    "            print(f\"   Field Match: {validation['field_match_percentage']:.1f}% ({validation['total_actual_fields']} actual vs {validation['total_expected_fields']} expected)\")\n",
    "            \n",
    "            if validation['missing_fields']:\n",
    "                print(f\"   Missing Fields ({len(validation['missing_fields'])}): {', '.join(validation['missing_fields'][:5])}{'...' if len(validation['missing_fields']) > 5 else ''}\")\n",
    "            \n",
    "            if validation['unexpected_fields']:\n",
    "                print(f\"   Extra Fields ({len(validation['unexpected_fields'])}): {', '.join(validation['unexpected_fields'][:5])}{'...' if len(validation['unexpected_fields']) > 5 else ''}\")\n",
    "                \n",
    "            if validation['has_utility_fields']:\n",
    "                print(\"   ‚úì Has utility fields (mode choice model)\")\n",
    "            if validation['has_probability_fields']:\n",
    "                print(\"   ‚úì Has probability fields (mode choice model)\")\n",
    "        \n",
    "        print(\"-\"*60)\n",
    "\n",
    "# Print the validation summary\n",
    "print_validation_summary(validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95277075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced validation DataFrame with CTRAMP documentation\n",
    "def create_validation_dataframe(validation_results: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a detailed validation DataFrame with CTRAMP documentation comparison\n",
    "    \"\"\"\n",
    "    validation_rows = []\n",
    "    \n",
    "    for validation in validation_results:\n",
    "        validation_rows.append({\n",
    "            'filename': validation['filename'],\n",
    "            'identified_ctramp_type': validation['identified_type'],\n",
    "            'file_description': validation['description'], \n",
    "            'validation_status': validation['validation_status'],\n",
    "            'appears_to_be_ctramp': validation['appears_to_be_ctramp'],\n",
    "            'field_match_percentage': validation['field_match_percentage'],\n",
    "            'total_actual_fields': validation['total_actual_fields'],\n",
    "            'total_expected_fields': validation['total_expected_fields'],\n",
    "            'missing_fields_count': len(validation['missing_fields']),\n",
    "            'unexpected_fields_count': len(validation['unexpected_fields']),\n",
    "            'has_utility_fields': validation['has_utility_fields'],\n",
    "            'has_probability_fields': validation['has_probability_fields'],\n",
    "            'missing_fields': '; '.join(validation['missing_fields'][:10]),  # Limit for CSV\n",
    "            'unexpected_fields': '; '.join(validation['unexpected_fields'][:10])  # Limit for CSV\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(validation_rows)\n",
    "\n",
    "# Create enhanced validation DataFrame\n",
    "validation_df = create_validation_dataframe(validation_results)\n",
    "\n",
    "print(\"üìä ENHANCED VALIDATION RESULTS\")\n",
    "print(f\"Total files analyzed: {len(validation_df)}\")\n",
    "print(f\"CTRAMP files identified: {validation_df['appears_to_be_ctramp'].sum()}\")\n",
    "print(\"\\nValidation Status Distribution:\")\n",
    "print(validation_df['validation_status'].value_counts())\n",
    "\n",
    "# Show top matches\n",
    "print(f\"\\nüèÜ TOP SCHEMA MATCHES:\")\n",
    "top_matches = validation_df[validation_df['appears_to_be_ctramp']].nlargest(5, 'field_match_percentage')\n",
    "for _, row in top_matches.iterrows():\n",
    "    print(f\"   {row['filename']}: {row['field_match_percentage']:.1f}% match ({row['identified_ctramp_type']})\")\n",
    "\n",
    "# Export enhanced validation results\n",
    "validation_output_file = \"ctramp_2023_validation_results.csv\"\n",
    "validation_df.to_csv(validation_output_file, index=False)\n",
    "print(f\"\\n‚úÖ Enhanced validation results exported to: {validation_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1237fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Checks with CTRAMP Knowledge\n",
    "def analyze_data_quality_with_ctramp_context(schema_results: dict, validation_results: list):\n",
    "    \"\"\"\n",
    "    Perform data quality analysis using CTRAMP domain knowledge\n",
    "    \"\"\"\n",
    "    print(\"üîç CTRAMP DATA QUALITY ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Find potential issues based on CTRAMP knowledge\n",
    "    issues_found = []\n",
    "    \n",
    "    for i, (file_info, validation) in enumerate(zip(schema_results['file_schemas'], validation_results)):\n",
    "        if file_info.get('error'):\n",
    "            continue\n",
    "            \n",
    "        filename = file_info['filename'] \n",
    "        columns = file_info.get('columns', [])\n",
    "        file_type = validation['identified_type']\n",
    "        \n",
    "        # Check for common CTRAMP data quality issues\n",
    "        file_issues = []\n",
    "        \n",
    "        # 1. Check for required ID fields\n",
    "        if validation['appears_to_be_ctramp']:\n",
    "            if 'hh_id' not in columns and file_type not in ['accessibilities.csv']:\n",
    "                file_issues.append(\"Missing household ID (hh_id)\")\n",
    "            \n",
    "            if file_type in ['personData', 'indivTourData', 'indivTripData', 'indivTripDataResim']:\n",
    "                if 'person_id' not in columns:\n",
    "                    file_issues.append(\"Missing person ID (person_id)\")\n",
    "                    \n",
    "            if file_type in ['indivTourData', 'jointTourData']:\n",
    "                if 'tour_id' not in columns:\n",
    "                    file_issues.append(\"Missing tour ID (tour_id)\")\n",
    "                    \n",
    "            if file_type in ['indivTripData', 'jointTripData', 'indivTripDataResim']:\n",
    "                if 'stop_id' not in columns:\n",
    "                    file_issues.append(\"Missing stop/trip ID (stop_id)\")\n",
    "        \n",
    "        # 2. Check for mode and time period fields\n",
    "        mode_fields = [col for col in columns if 'mode' in col.lower()]\n",
    "        period_fields = [col for col in columns if 'period' in col.lower()]\n",
    "        \n",
    "        if validation['appears_to_be_ctramp'] and file_type in ['indivTourData', 'indivTripData', 'jointTourData', 'jointTripData']:\n",
    "            if not mode_fields:\n",
    "                file_issues.append(\"No mode fields found\")\n",
    "            if not period_fields:\n",
    "                file_issues.append(\"No time period fields found\")\n",
    "        \n",
    "        # 3. Check for utility/probability field consistency\n",
    "        util_fields = [col for col in columns if col.startswith('util_')]\n",
    "        prob_fields = [col for col in columns if col.startswith('prob_')]\n",
    "        \n",
    "        if util_fields and not prob_fields:\n",
    "            file_issues.append(\"Has utility fields but missing probability fields\")\n",
    "        elif prob_fields and not util_fields:\n",
    "            file_issues.append(\"Has probability fields but missing utility fields\")\n",
    "        elif util_fields and prob_fields:\n",
    "            if len(util_fields) != len(prob_fields):\n",
    "                file_issues.append(f\"Utility/probability field count mismatch ({len(util_fields)} vs {len(prob_fields)})\")\n",
    "        \n",
    "        # 4. Check for geographic fields (MAZ/TAZ)\n",
    "        geo_fields = [col for col in columns if 'mgra' in col.lower() or 'taz' in col.lower() or 'tap' in col.lower()]\n",
    "        if validation['appears_to_be_ctramp'] and file_type not in ['householdData', 'personData']:\n",
    "            if not geo_fields:\n",
    "                file_issues.append(\"No geographic fields (MGRA/TAZ/TAP) found\")\n",
    "        \n",
    "        # 5. Sample size estimation\n",
    "        if file_info.get('shape'):\n",
    "            rows = file_info['shape'][0]\n",
    "            if validation['appears_to_be_ctramp']:\n",
    "                if file_type in ['householdData'] and rows < 100000:\n",
    "                    file_issues.append(f\"Low household count ({rows:,}) - expected 100K+ for Bay Area\")\n",
    "                elif file_type in ['personData'] and rows < 200000:\n",
    "                    file_issues.append(f\"Low person count ({rows:,}) - expected 200K+ for Bay Area\")\n",
    "                elif file_type in ['indivTripData'] and rows < 1000000:\n",
    "                    file_issues.append(f\"Low trip count ({rows:,}) - expected 1M+ for Bay Area\")\n",
    "        \n",
    "        if file_issues:\n",
    "            issues_found.append({\n",
    "                'filename': filename,\n",
    "                'file_type': file_type,\n",
    "                'issues': file_issues,\n",
    "                'issue_count': len(file_issues)\n",
    "            })\n",
    "    \n",
    "    # Report findings\n",
    "    if issues_found:\n",
    "        print(f\"‚ö†Ô∏è  Found potential issues in {len(issues_found)} files:\")\n",
    "        for issue_info in issues_found:\n",
    "            print(f\"\\nüìÑ {issue_info['filename']} ({issue_info['file_type']})\")\n",
    "            for issue in issue_info['issues']:\n",
    "                print(f\"   ‚Ä¢ {issue}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No major data quality issues detected!\")\n",
    "    \n",
    "    return issues_found\n",
    "\n",
    "# Run data quality analysis\n",
    "quality_issues = analyze_data_quality_with_ctramp_context(schema_results, validation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1433c3",
   "metadata": {},
   "source": [
    "## üìã CTRAMP Documentation Summary\n",
    "\n",
    "### Key File Types Expected:\n",
    "- **Individual Tours** (`indivTourData_*.csv`): Person-level tours with mode choice utilities\n",
    "- **Individual Trips** (`indivTripData_*.csv`): Person-level trips with detailed geography \n",
    "- **Joint Tours/Trips** (`jointTourData_*.csv`, `jointTripData_*.csv`): Household joint travel\n",
    "- **Household Data** (`householdData_*.csv`): HH characteristics and model results\n",
    "- **Person Data** (`personData_*.csv`): Person attributes and choice model results\n",
    "- **Location Choice** (`wsLocResults_*.csv`): Work/school location model results\n",
    "- **Auto Ownership** (`aoResults*.csv`): Vehicle ownership model results\n",
    "- **Accessibility** (`accessibilities.csv`): Zone-level accessibility measures\n",
    "\n",
    "### Mode Codes (1-17):\n",
    "1-2: Drive Alone (Free/Pay), 3-8: Shared Ride (2/3+ persons, GP/HOV/Pay), 9-10: Walk/Bike, \n",
    "11-14: Transit (Walk/PNR/KNR), 15-17: Taxi/TNC/School Bus\n",
    "\n",
    "### Time Periods (1-40):\n",
    "30-minute periods from 3:00 AM to 3:00 AM next day, with Period 1 = 3:00-5:00 AM\n",
    "\n",
    "### Important Notes:\n",
    "- Files may have iteration numbers in names (e.g., `_1.csv`, `_2.csv`)\n",
    "- Tour/trip files have mode choice utilities (`util_1` to `util_17`) and probabilities (`prob_1` to `prob_17`)\n",
    "- Geographic fields use MGRA (microzones), TAP (transit access points) \n",
    "- Sample rates may vary by iteration for memory management\n",
    "- Joint travel files have one record per trip (not per person)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
